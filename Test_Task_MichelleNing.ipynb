{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Task\n",
    "\n",
    "# Analyze the organization name recognition system and dataset annotations\n",
    "\n",
    "\n",
    "\n",
    "### The Road Ahead\n",
    "\n",
    "Here, break the notebook into separate steps.\n",
    "\n",
    "* [Step 1](#step1): Load Enron Dataset\n",
    "* [Step 2](#step2): Analyze spacy model performace on Enron file\n",
    "* [Step 3](#step3): Load OntoNotes Dataset\n",
    "* [Step 4](#step4): Analyze spacy model performace on OntoSentences file\n",
    "\n",
    "---\n",
    "<a id='step1'></a>\n",
    "## Step 1: Load Enron Dataset\n",
    "\n",
    "### Load Enron Dataset\n",
    "Open and load EnronSentences.json file\n",
    "The type of the data is list and length = 310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "310\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Open JSON file\n",
    "with open('EnronSentences.json') as json_file:\n",
    "    Enron_data = json.load(json_file)\n",
    "    \n",
    "    print(type(Enron_data))\n",
    "    print(len(Enron_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take the first email as an example.\n",
    "#### Email is read in dict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"I have made this correction in EPMI's CEC filing and resubmitted it.\", 'machine_entities': [[31, 35, 'ORG', 'machine'], [38, 41, 'ORG', 'machine']], 'human_entities': {'44357992': [[31, 35, 'ORG', 'machine']]}}\n",
      "Enron_data[4] type: <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# print an email and check content\n",
    "print(Enron_data[4])\n",
    "print(\"Enron_data[4] type:\",type(Enron_data[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check keys and its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text', 'machine_entities', 'human_entities'])\n",
      "I have made this correction in EPMI's CEC filing and resubmitted it.\n",
      "<class 'list'>   [[31, 35, 'ORG', 'machine'], [38, 41, 'ORG', 'machine']]\n",
      "[31, 35, 'ORG']\n",
      "machine\n"
     ]
    }
   ],
   "source": [
    "# check each dictionary keys and value\n",
    "print(Enron_data[4].keys())\n",
    "input_text = Enron_data[4]['text']\n",
    "output = Enron_data[4]['machine_entities']\n",
    "print(input_text)\n",
    "print(type(output), \" \", output)\n",
    "print(output[0][:3])\n",
    "print(output[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[31, 35, 'ORG', 'machine'], [38, 41, 'ORG', 'machine']]\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "machine_output = Enron_data[4]['machine_entities']\n",
    "human_output = Enron_data[0]['human_entities']\n",
    "print(machine_output)\n",
    "print(human_output)\n",
    "#print(human_output.values())\n",
    "#list(human_output.values())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Analyze spacy model performace on Enron file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the annotation by machine and human\n",
    "\n",
    "#### First to figur out how many emails were edited by human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_each_email(n):\n",
    "    \"\"\"\n",
    "    Read each email and return:\n",
    "        - machine annotated output\n",
    "        - human annotated output\n",
    "    \"\"\"\n",
    "    output_machine = Enron_data[n]['machine_entities']\n",
    "    output_human = Enron_data[n]['human_entities']\n",
    "    result_human = []\n",
    "    \n",
    "    if output_human != {}:\n",
    "        \n",
    "        output_human_dict = output_human\n",
    "        result_human = list(output_human_dict.values())[0]\n",
    "\n",
    "    return output_machine, result_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are:  59  are corrected by human.\n",
      "\n",
      "[4, 8, 24, 29, 30, 33, 40, 52, 65, 72, 82, 104, 105, 118, 119, 121, 125, 131, 132, 135, 139, 140, 153, 154, 157, 165, 168, 169, 170, 172, 173, 174, 176, 177, 178, 181, 188, 196, 197, 218, 230, 231, 244, 245, 252, 256, 269, 272, 287, 293, 297, 299, 300, 301, 302, 303, 307, 308, 309]\n"
     ]
    }
   ],
   "source": [
    "# Figur out how many emails are edited by human.\n",
    "no_change_email = 0\n",
    "correct_index = []\n",
    "edited_index = []\n",
    "\n",
    "for i in range(len(Enron_data)):\n",
    "    # get machine output and human output\n",
    "    output_machine, result_human = read_each_email(i)\n",
    "    if result_human == []:\n",
    "        no_change_email += 1\n",
    "        correct_index.append(i)\n",
    "    else:\n",
    "        edited_index.append(i)\n",
    "        \n",
    "edited_number = len(Enron_data) - no_change_email\n",
    "print(\"There are: \", edited_number, \" are corrected by human.\\n\")\n",
    "print(edited_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Focused on these email corrected by human\n",
    "\n",
    "Figur out what were considered as false negative name.\n",
    "\n",
    "It is important to know which names spaCy considered it not a ORG name but human annotated as ORG.\n",
    "\n",
    "It is also good to know:\n",
    "    * names that spaCy correctly annotated\n",
    "    * names that spaCy annotated as ORG but human deleted\n",
    "    * names that spaCy missed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word(value, text):\n",
    "    \"\"\"\n",
    "    According to the start and end position to find the ORG name\n",
    "    \"\"\"\n",
    "    start = value[0]\n",
    "    end = value[1]\n",
    "    word = text[start:end]\n",
    "    return word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Enron': 17, 'ENA': 9, 'ISDA': 5, 'Nicor': 4, 'Texas': 3, 'Cinergy': 3, 'FYI': 3, 'VAR': 3, 'CEC': 2, 'Sierra': 2, 'GISB': 2, 'Central': 2, 'HPL': 2, 'MILLS': 2, 'Yahoo': 2, 'The': 2, 'EOTT': 2, 'NDA': 2, 'Harvard': 2, 'CPUC': 2, 'Amazon.com': 2, 'Master': 2, 'ProCaribe': 2, 'Socal': 2, 'Waddington': 2, 'Containerisation': 2, 'Model': 2, 'PG&E': 2, 'Legal': 2, 'VMAC': 2, 'ISO': 2, 'Real': 2, 'CNN': 2, 'NNG': 1, 'Westheimer': 1, 'Gingerbread': 1, 'Oxley': 1, 'East': 1, 'ICAP': 1, 'Stinson/Vince': 1, 'Client': 1, 'Confirmation': 1, 'Ken': 1, 'Principal-Protected*': 1, 'Trust': 1, 'Nasdaq-100': 1, 'Ambac': 1, \"Moody's\": 1, 'AAA': 1, 'Standard': 1, 'Enerfax': 1, 'GMT-06:00': 1, 'Global': 1, 'Seller': 1, 'PEPL': 1, 'TE': 1, 'Submit': 1, 'Christi': 1, 'Dominion': 1, 'ESA': 1, 'RAC': 1, 'Eastern': 1, 'PPT': 1, 'Supervisor': 1, 'SHRM': 1, 'CRRA': 1, 'TransPecos': 1, 'FBI': 1, 'CommodityLogic': 1, 'ASAP': 1, 'Peoples': 1, 'EFF_DT': 1, 'Jesse': 1, 'Specialist': 1, 'Logistics': 1, 'Associates': 1, 'Morton': 1, 'GCP': 1, 'HSC': 1, 'Kinder': 1, 'Stan': 1, 'West': 1, 'CSA': 1, 'CIAC': 1, 'Oil-NG-Hedge-Spec': 1, 'Wal-Mart': 1, 'SoCal': 1, 'Corp.': 1, 'NDA-Credit2B.com': 1, 'DealBench': 1, 'LPG': 1, '~2900': 1, 'Bracewell': 1, 'Marketing': 1, 'GPG': 1, 'Level': 1, 'Craver': 1, 'Reserve': 1, 'Financial': 1, 'Hearing': 1, 'GTC': 1, 'Delphi': 1, 'Bobinchuck': 1, 'Louis': 1, 'Travelocity.com': 1, 'Border': 1, 'Credit': 1, 'ECT': 1, 'IROQ': 1, 'CI': 1, 'Kenneth.Wong@gm.com': 1, 'Reuters': 1, 'Shelley': 1, 'Japanese': 1, 'EES': 1, 'Direct': 1, 'Truluck': 1, 'Kelly': 1, 'Board': 1, 'AEP': 1, 'Tracy': 1, 'Gallup': 1, 'Agave': 1, 'Dennis': 1, 'CLEC': 1, 'Swidler': 1, 'Unit': 1, 'Zone': 1, 'Kimberly': 1, 'Katherine': 1, 'Canadian': 1, 'Cordes': 1, 'Paul,': 1, 'Topock': 1, 'Lotus': 1, 'Lavarato': 1, 'DETM': 1, 'Groups': 1, 'Independent': 1, 'Corry': 1, 'OATI': 1, 'Enron01': 1, 'CFTC': 1, 'Attached': 1, 'NNG/TW': 1, 'WordPerfect': 1, 'Rich': 1, 'Kansas': 1, 'RisktRac': 1, 'Howzabout': 1, 'Business': 1, 'Weekly': 1, 'Theresa': 1, 'Weil': 1, 'Baker': 1, 'LiveLink': 1, 'LV': 1, 'Akilesh': 1, '\\\\\\\\nahou-psecn01v': 1, 'Essie': 1, 'Leon': 1, 'xferring': 1, 'Buffet': 1, 'Fastow': 1, 'Megawatt-Daily': 1, 'EOL': 1, 'US': 1, 'Rob': 1, 'Legislature': 1, 'Tri': 1, 'New': 1, 'Transwestern': 1, 'TW': 1, 'Minnis,': 1, 'Burlington': 1, 'EOT': 1, 'Christie,': 1, 'Suzanne': 1, 'BoozAllen': 1, 'Antitrust': 1, 'Nymex': 1, 'EIM': 1, 'CGAS': 1, 'Rosalee': 1, 'Intergen': 1, 'EDI': 1, 'GMC': 1, 'Franklin': 1, 'Illinois': 1, 'Dunn': 1, 'AG': 1, 'Zimin': 1, 'LA': 1, 'Morrow': 1, 'BPA': 1, 'Annette': 1, 'Lone': 1, 'Plant': 1, 'Contact': 1, 'Maclaren': 1, 'Allen': 1, 'Meredith': 1, 'Sports': 1})\n"
     ]
    }
   ],
   "source": [
    "name_found_correct = []\n",
    "\n",
    "for n in correct_index:\n",
    "\n",
    "    input_text = Enron_data[n]['text']\n",
    "    output_machine = Enron_data[n]['machine_entities']\n",
    "    \n",
    "    for value in output_machine:\n",
    "        name_correct = find_word(value, input_text)\n",
    "        name_found_correct.append(name_correct)\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "counts_name_found = Counter(name_found_correct)\n",
    "print(counts_name_found)\n",
    "#vocab_found = sorted(counts_name_found, key=counts_name_found.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated name by machine is deleted by human: \n",
      "\n",
      "Counter({'FERC': 2, 'ROFR': 2, 'CEC': 1, 'ABB': 1, 'GE': 1, 'ISDA': 1, 'PEPL': 1, 'BNP/EML': 1, 'Mariella': 1, 'LPG': 1, 'PEP': 1, 'Supervisor': 1, 'January': 1, 'DPC': 1, 'LNG': 1, 'K#66940': 1, 'Interstate': 1, 'California': 1, 'Central': 1, 'LOI': 1, \"L/C's\": 1, 'L/C': 1, 'Harry': 1, '6/13/01': 1, 'MGI': 1, 'FYI': 1, 'Master': 1, 'Weekly': 1, 'NBSK': 1, 'CLE': 1, 'Underwriting': 1, 'IV': 1, 'FX': 1, 'GPG': 1, 'NBPL': 1, 'Veroinca': 1, 'GISB': 1, 'GMT-06:00': 1, 'INGAA': 1, 'BPA': 1, 'Client': 1, 'Outages': 1, 'BB': 1, 'NGX': 1, 'ENA': 1, 'The': 1}) \n",
      "\n",
      "spaCy missed names: \n",
      "\n",
      "Counter({'PEPL': 3, 'PEP': 3, '6/13/01': 3, 'Client': 3, 'GE': 2, 'FERC': 2, 'Supervisor': 2, 'Enron': 2, 'K#66940': 2, 'California': 2, 'UBS': 2, 'Master': 2, 'ROFR': 2, 'ISO': 2, 'The': 2, 'EnronOnline': 1, 'ISDA': 1, 'CO2': 1, 'Henry': 1, 'BNP/EML': 1, 'Hector': 1, 'LPG': 1, '2600MT': 1, 'LNG': 1, 'LOI': 1, \"L/C's\": 1, 'L/C': 1, 'AEP': 1, 'MGI': 1, 'FYI': 1, 'Weekly': 1, 'MAC': 1, 'NBSK': 1, 'CLEC': 1, 'CLE': 1, 'EC': 1, 'IV': 1, 'FX': 1, 'GPG': 1, 'NBPL': 1, 'GISB': 1, 'INGAA': 1, 'BPA': 1, 'Mobil': 1, 'Outages': 1, 'Twanda': 1, 'What t': 1, 'ENA': 1, 'Pau': 1})\n"
     ]
    }
   ],
   "source": [
    "name_found_wrong = []\n",
    "name_missed = []\n",
    "\n",
    "for n in edited_index:\n",
    "    input_text = Enron_data[n]['text']\n",
    "    output_machine, output_human = read_each_email(n)\n",
    "    \n",
    "    for value in output_machine:\n",
    "        # find the name\n",
    "        name = find_word(value, input_text)\n",
    "        \n",
    "        if value in output_human:\n",
    "            name_found_correct.append(name)\n",
    "        else:\n",
    "            name_found_wrong.append(name)\n",
    "            \n",
    "    for result in output_human:\n",
    "        if not result in output_machine:\n",
    "            name = find_word(value, input_text)\n",
    "            name_missed.append(name)\n",
    "\n",
    "counts_name_wrong = Counter(name_found_wrong)\n",
    "print(\"Annotated name by machine is deleted by human: \\n\")\n",
    "print(counts_name_wrong, \"\\n\")\n",
    "\n",
    "counts_name_missed = Counter(name_missed)\n",
    "print(\"spaCy missed names: \\n\")\n",
    "print(counts_name_missed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compared Names\n",
    "\n",
    "Based on the above three word lists, we can get some results.\n",
    "* In name_found_correct list, some words need to be deleted.\n",
    ">- Like: 'FYI', 'Central', 'Jesse', 'Dennis', etc\n",
    "\n",
    "* There are some words appeared in both name_found_correct list and name_found_wrong list, like 'ENA', 'CEC', 'PBA', 'Morton', 'EOT', 'PG&E'. \n",
    ">- When human editored the annotation, human made mistakes. Take 'CEC' as an example, CEC are annotated as ORG name for twice, however, it is deleted by human for one time.\n",
    "    \n",
    "* There are some words appeared in name_found_wrong list and name_missed list, like 'PEP', 'LNG', 'INGAA', 'NBSK'\n",
    ">- Sometimes, human recognized these words as ORG, sometimes, not\n",
    ">- ABB is a Company, however, it is deleted by editors.\n",
    "\n",
    "* There are some words appeared in all three list, like 'PEPL', 'LPG'\n",
    ">- This situation would make the result confusing.\n",
    "\n",
    "Human editors are also helpful.\n",
    "* From name_found_wrong list, editors really removed words correctly, such as\n",
    ">- words means name, location, date, time\n",
    ">- words meaningless (like K#66940)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "## Step 3: Load OntoNotes Dataset\n",
    "\n",
    "### Load OntoNotes.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "# Open JSON file\n",
    "with open('OntoNotes.json') as json_file:\n",
    "    Onto_data = json.load(json_file)\n",
    "    \n",
    "    print(type(Onto_data))\n",
    "    print(len(Onto_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The head of the Palestinian Television and Radio has been shot dead in the Gaza Strip .\n"
     ]
    }
   ],
   "source": [
    "input_text = str(Onto_data[0][0])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The head of the Palestinian Television and Radio has been shot dead in the Gaza Strip .', {'entities': [[12, 48, 'ORG'], [71, 85, 'GPE']]}]\n",
      "Onto_data[0] type: <class 'list'>\n",
      "The head of the Palestinian Television and Radio has been shot dead in the Gaza Strip .\n",
      "[[12, 48, 'ORG'], [71, 85, 'GPE']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[12, 48, 'ORG']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Onto_data[0])\n",
    "print(\"Onto_data[0] type:\",type(Onto_data[0]))\n",
    "print(Onto_data[0][0])\n",
    "output_Onto = Onto_data[0][1]['entities']\n",
    "print(output_Onto)\n",
    "\n",
    "output_test = []\n",
    "for value in output_Onto:\n",
    "    if value[2] == 'ORG':\n",
    "        output_test.append(value)\n",
    "output_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import spacy and analyze the first email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12, 38, 'ORG'], [43, 48, 'ORG']]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(input_text)\n",
    "output_spacy = []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"ORG\":\n",
    "        output_spacy.append([ent.start_char,ent.end_char,ent.label_])\n",
    "print(output_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "result = output[0][:3] == output_spacy[0]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predict_from_spacy(input_text):\n",
    "    \"\"\"\n",
    "    Spacy is used to annotate ORG name from email content\n",
    "    Return annotated result\n",
    "    \"\"\"\n",
    "    doc = nlp(input_text)\n",
    "    output_spacy = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"ORG\":\n",
    "            output_spacy.append([ent.start_char,ent.end_char,ent.label_])\n",
    "    \n",
    "    return output_spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12, 38, 'ORG'], [43, 48, 'ORG']]\n"
     ]
    }
   ],
   "source": [
    "# test get_predict_from_spacy function\n",
    "output_spacy = get_predict_from_spacy(input_text)\n",
    "print(output_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step4'></a>\n",
    "## Step 4: Analyze spacy model performace on OntoSentences file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the value got from spacy and file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_match(output, predict):\n",
    "    \"\"\"\n",
    "    Check weather Spacy's predict has the same result of target\n",
    "    output - Enron result\n",
    "    predict - Spacy's predict\n",
    "    \"\"\"\n",
    "    if predict == output:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def correct_find(output, predict):\n",
    "    \"\"\"\n",
    "    find whether the names annotated by spacy are in the range of target\n",
    "    output - Enron result\n",
    "    predict - Spacy's predict\n",
    "    \n",
    "    The name annotated by spacy sometime starts or ends with different position from target.\n",
    "    However, sometimes, a part of the name is annotated by spacy.\n",
    "    This situation is considered as spacy found name correctly.\n",
    "    \"\"\"\n",
    "    find_name = 0\n",
    "    for value_predict in predict:\n",
    "        for value_output in output:\n",
    "            if value_predict[0] > value_output[1] or value_predict[1] < value_output[0]:\n",
    "                find_name += 0\n",
    "            else:\n",
    "                find_name += 1\n",
    "    return find_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_output_from_OntoNotes(n):\n",
    "    info = Onto_data[n]\n",
    "    input_text = str(Onto_data[n][0])\n",
    "    output = Onto_data[n][1]['entities']\n",
    "    output_Onto = []\n",
    "    for value in output:\n",
    "        if value[2] == 'ORG':\n",
    "            output_Onto.append(value)\n",
    "    \n",
    "    return input_text, output_Onto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete_correct_number Positive:  1397\n",
      "All position found:  190\n",
      "True Positive:  1587\n",
      "True Negative:  3341\n",
      "with false positive number:  11\n",
      "with false negative number:  61\n"
     ]
    }
   ],
   "source": [
    "complete_correct_number = 0\n",
    "position_find_number = 0\n",
    "false_positive_number = 0\n",
    "false_negative_number = 0\n",
    "ture_negative_number = 0\n",
    "\n",
    "for n in range(len(Onto_data)):\n",
    "    input_text, output_Onto = read_output_from_OntoNotes(n)\n",
    "    len_output = len(output_Onto)\n",
    "    output_spacy = get_predict_from_spacy(input_text)\n",
    "    len_spacy = len(output_spacy)\n",
    "    \n",
    "    if complete_match(output_Onto, output_spacy) == 1:\n",
    "\n",
    "        if output_spacy == []:\n",
    "            ture_negative_number += 1\n",
    "        else:\n",
    "            complete_correct_number += 1\n",
    "        #print(i, \" Completely Match OntoSentences: \", output_Onto, \" spacy: \", output_spacy)\n",
    "    elif correct_find(output_Onto, output_spacy) == len(output_Onto):\n",
    "        position_find_number += 1     \n",
    "    \n",
    "    else:\n",
    "        if len_spacy >= len_output:  # spacy has false positive\n",
    "            false_positive_number += 1\n",
    "        \n",
    "        elif len_spacy < len_output:  # spacy has false negative\n",
    "            false_negative_number += 1\n",
    "            #print(input_text)\n",
    "            #print(n, \" OntoSentences: \", output_Onto, \" spacy: \", output_spacy)\n",
    "                \n",
    "print(\"complete_correct_number Positive: \", complete_correct_number)\n",
    "print(\"All position found: \", position_find_number)\n",
    "print(\"True Positive: \", complete_correct_number + position_find_number)\n",
    "print(\"True Negative: \", ture_negative_number)\n",
    "print(\"with false positive number: \", false_positive_number)\n",
    "print(\"with false negative number: \", false_negative_number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here use the following matrix to analyze results\n",
    "\n",
    "|               |Predict Positive        | Predict Negative           |\n",
    "| ------------- |:-------------:| -----:|\n",
    "| Positive      | True Positive | False Negative |\n",
    "| Negative      | False Positive| True Negative  |\n",
    "\n",
    "Based on the result from above step, it is easy to find:\n",
    "\n",
    "there are 1397 emails correctly annotate ORG by spacy\n",
    "there are 190 emaild can include then names needed to be annotated by spacy\n",
    "there are 3341 emails don't have ORG name checked by spacy, the same as Enton file\n",
    "there are 11 emails annotated with wrong ORG name by spacy\n",
    "there are 61 emails which can not be annotated ORG name by spacy\n",
    "hence,\n",
    "    - True Positive = 1397 + 190 = 1587\n",
    "    - True Negative = 3341\n",
    "    - False Positive = 11\n",
    "    - False Negative = 61\n",
    "When only consider correct results, then:\n",
    "\n",
    "    - Accuracy = (True Positive + True Negative)/ Total = (1587 + 3341) / 5000 = 0.99\n",
    "    - Recall = True Positive/(True Positive + False Negative) = 1587/(1587+61) = 0.96\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
